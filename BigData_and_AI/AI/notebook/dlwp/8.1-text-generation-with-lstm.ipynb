{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with LSTM\n",
    "\n",
    "This notebook contains the code samples found in Chapter 8, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "[...]\n",
    "\n",
    "## Implementing character-level LSTM text generation\n",
    "\n",
    "\n",
    "Let's put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a \n",
    "language model. You could use any sufficiently large text file or set of text files -- Wikipedia, the Lord of the Rings, etc. In this \n",
    "example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model \n",
    "we will learn will thus be specifically a model of Nietzsche's writing style and topics of choice, rather than a more generic model of the \n",
    "English language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Let's start by downloading the corpus and converting it to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 2s 3us/step\n",
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we will extract partially-overlapping sequences of length `maxlen`, one-hot encode them and pack them in a 3D Numpy array `x` of \n",
    "shape `(sequences, maxlen, unique_characters)`. Simultaneously, we prepare a array `y` containing the corresponding targets: the one-hot \n",
    "encoded characters that come right after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200278\n",
      "Unique characters: 57\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Our network is a single `LSTM` layer followed by a `Dense` classifier and softmax over all possible characters. But let us note that \n",
    "recurrent neural networks are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in \n",
    "recent times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = keras.layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the language model and sampling from it\n",
    "\n",
    "\n",
    "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
    "\n",
    "* 1) Drawing from the model a probability distribution over the next character given the text available so far\n",
    "* 2) Reweighting the distribution to a certain \"temperature\"\n",
    "* 3) Sampling the next character at random according to the reweighted distribution\n",
    "* 4) Adding the new character at the end of the available text\n",
    "\n",
    "This is the code we use to reweight the original probability distribution coming out of the model, \n",
    "and draw a character index from it (the \"sampling function\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1) # 多项式分布采样\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures \n",
    "after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of \n",
    "temperature in the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 09:39:02.581187 140125266151232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Train on 200278 samples\n",
      "200278/200278 [==============================] - 100s 498us/sample - loss: 1.9630\n",
      "--- Generating with seed: \"hat his opinion of others is and their opinion of him. final\"\n",
      "------ temperature: 0.2\n",
      "hat his opinion of others is and their opinion of him. finally in the presenting to all the ending the expersing and the seastion of the expersing and presenting and the presenting to and the sensition of the sensessing to the earthered to the eres to the presenting to the essicity of the earthered to the ending of the expersing and free the seems the presere of the ears of the endired and pressions and the present in the present and and presenting in the \n",
      "------ temperature: 0.5\n",
      "ns and the present in the present and and presenting in the existour propers of the and stringter and the will to he will properstarity, and the seasting the souls alloot actarted of the frees the histo it is new the greats and expersion and and same his of the plilosophers in the essing of the rearn of the seaster, and it is alto all the greation of the histow and the ending seastions and the precised, and wishout and the artart of the presenting and and \n",
      "------ temperature: 1.0\n",
      "cised, and wishout and the artart of the presenting and and gemman, without the opprelige, are\n",
      "mears of the pricist repliled in his spirit to restins spirit from consy3ting that hadaunt\n",
      "religion-ithical altod pracisy than has sievical the indevinage knowe gde, vilthis aleforal, differalce,\n",
      " i. we arantly in, enemed dithe keligations sieps of councersgan be loocering the earthess procected alloes of pursoable opidiving to allaving tabs were essequentines--w\n",
      "------ temperature: 1.2\n",
      "f pursoable opidiving to allaving tabs were essequentines--will bild-lyter--respoud are was the disirtisal.\n",
      "\n",
      "omick snew menge. the picsly. in the infleends and\n",
      "leasted trated! of als\n",
      "butds, also imhac. actaiting haod to it\n",
      "oar forthings\n",
      "so the not to dealneness,, the erans-aloshation it\n",
      "the vild demalse shet, \"fully and weol earthuse- to moral toant siding ormined il excerticul entingly were concectation. that it he fairs. on\n",
      "the grodutionsly, aad e realie\n",
      "epoch 2\n",
      "Train on 200278 samples\n",
      "200278/200278 [==============================] - 99s 493us/sample - loss: 1.6125\n",
      "--- Generating with seed: \"s which\n",
      "manifests itself repeatedly, but just here is where \"\n",
      "------ temperature: 0.2\n",
      "s which\n",
      "manifests itself repeatedly, but just here is where a sense of the same the self of the sense of the superath the same the superath and the more and the sense of the supermist the sense of the same the more the saudity that is the sense of the sense of the fature of the same the same the same the sense of the same the superath the suffering the supermise in the suffering the same the same and the same the same the sense of the suffering the same th\n",
      "------ temperature: 0.5\n",
      "and the same the same the sense of the suffering the same them that it them all the also the hand, in the conducion and that so that the charm\n",
      "has the man in the satisting in the supercompartion that\n",
      "one man in the reason, as the refreence the world of the present in such rations the world deep to a ourder in the superater to be all conception of the present be be be man the subject, the pholos of the readanted one has elefuled that one has more the said t\n",
      "------ temperature: 1.0\n",
      " the readanted one has elefuled that one has more the said to to badry and\n",
      "doing theag.\n",
      "\n",
      "\n",
      "2be\n",
      "fulth anding\n",
      "remined by the great still\n",
      "itself that is\n",
      "fort signery in avery to suce x.oncing ustitud man in they my difficunt lamong\n",
      "day from whiming symial sentiry betoloding but onebad man ungrabosts which\"uff again. b, eterny a toon to this kent =sward preseife\n",
      "of firth farmly that righty\" ard\n",
      "\"?deeps durinates\n",
      "and  work feury we kinds of these nevered with\n",
      "wh\n",
      "------ temperature: 1.2\n",
      " durinates\n",
      "and  work feury we kinds of these nevered with\n",
      "which theing torture love phopoo, withle with-i, demisent volr ffancic out in the still thinds.=     , theys knowlf se, commen--that thelretion,,, men [8kening edeacce, these verytry,\n",
      "our rancliausy, your allfoinn,\n",
      "sympathy its\n",
      "altain in different trims hands evolity.\n",
      "we poter entorcelnful celtured ye kind agassionating pleass of desurfy?--them science concessive, in clartmex ourselves ocher themder\n",
      "epoch 3\n",
      "Train on 200278 samples\n",
      "200278/200278 [==============================] - 99s 495us/sample - loss: 1.5237\n",
      "--- Generating with seed: \"hat is alone proper for the\n",
      "present: namely, the collection \"\n",
      "------ temperature: 0.2\n",
      "hat is alone proper for the\n",
      "present: namely, the collection of the same are souls and not that the same to the fact of the man and all the extractions of the sures of the man and things and present in the same tark of the strange of the same thing that is that which the superits and which the same tark of the man and present in the same thing that is that the superits and present in the same taske of the same manifests of the same taken that is the superit\n",
      "------ temperature: 0.5\n",
      " of the same manifests of the same taken that is the superits that the made of presences of influence of a find of the great has one in the can always has a factured such the present complen of flowing will of the\n",
      "goal extreld that he mankind that the germanism and restrumation of regard in the same world and schooled became of him who are things, or that is the same time gaintly that itself, that all the ethical and sone\n",
      "recains that it is presentis to th\n",
      "------ temperature: 1.0\n",
      " all the ethical and sone\n",
      "recains that it is presentis to the paptice, as were\n",
      "condownce, us preed, thoughctanders, the speet and plays that immerie,\" not in mack of in it, advantaurgies\n",
      "such so funghare, that he\n",
      "mast in, weatteriout and companding, on the equising eduorotress of which hast beton\n",
      "for horgubled of farter suraoroly nowadays frum originfuled\n",
      "it. pencernary amfoc; it hepreble, serithal inexurese,\n",
      "calle\"; he metaped that que\"nestifue, urpregina\n",
      "------ temperature: 1.2\n",
      "l inexurese,\n",
      "calle\"; he metaped that que\"nestifue, urpreginated asnect; there you make\n",
      "hearty of\n",
      "the sextemsnist\n",
      "wild\n",
      "in \n",
      "herenefn or perhaps of abtegen gilus haw haf in view tobedy but of dound, always moster from a lack.\n",
      "\n",
      "f\n",
      "reltary,\n",
      "mak,ness.=--wan lowe. oneself (and that manken and pupise on nationally, berisodesiaisms are ow all of ell tener: herprititye,\n",
      "homestant and medivally,\" them matt that always,ic olice dismner latty\n",
      "humanity became beon nabe d\n",
      "epoch 4\n",
      "Train on 200278 samples\n",
      "200278/200278 [==============================] - 90s 448us/sample - loss: 1.4786\n",
      "--- Generating with seed: \"mum, in\n",
      "the form of which \"faith\" comes to it. modern men, w\"\n",
      "------ temperature: 0.2\n",
      "mum, in\n",
      "the form of which \"faith\" comes to it. modern men, which is always the men, the same the same to the sentiment to be all the same the same that the more to the sensesting the same the moral the same the same the contrary and the same to which is always the moral the same the moral and the same the same to which is and the moral intenteration and where the men of the men of the contrary to deschanally and men the more the same to the faction of the \n",
      "------ temperature: 0.5\n",
      "deschanally and men the more the same to the faction of the called under the mele, then the classion the same the enthing the sentiment, and the world of the moral extere which is to be any gradicate the devilsion\n",
      "much that which the same to be modest in the delights of men, in the more and enthury of\n",
      "morality of his contralled in its all the contrary of heart have been of the \"man as a thing the same of the sentiment interrances of the man and into the sa\n",
      "------ temperature: 1.0\n",
      "same of the sentiment interrances of the man and into the say, the moust years, perpealising, when the\n",
      "reveloping, fin theoredood, one\n",
      "knowily\n",
      "upon to be man of the clear which sfore from edemost to adord--as the flactly elpreist\n",
      "the sentive berome away humated\n",
      "by feeling the\n",
      "facuting, a fear antept of extrafalication, and mad.\n",
      "\n",
      "adness, it is or freelest on: they wistess;\n",
      "imlimee of conversoly, conscience, the heal it all\n",
      "diver renigent--that o let drew fr\n",
      "------ temperature: 1.2\n",
      "nscience, the heal it all\n",
      "diver renigent--that o let drew from me federation, upon\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "befencal eusomed, lby from sabed.e\n",
      "alse justly than imbord it knowledge', when is be\": show grisul from remist. and sharination. the mo everyy histsible moddemital and onibud ?ost of hy disconderate and dispisingly of mound narbory, \"serivate\n",
      "isably perhaps, the\n",
      "dee, too, must\n",
      "\"debile or, ruth endrands to\n",
      "on\n",
      "egerneed.\n",
      "musx certain\n",
      "russiangation in the dangively stsoi schame\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in \n",
    "particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text \n",
    "becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible (such as \n",
    "\"eterned\" or \"troveration\"). With a high temperature, the local structure starts breaking down and most words look like semi-random strings \n",
    "of characters. Without a doubt, here 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment \n",
    "with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting.\n",
    "\n",
    "Note that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and \n",
    "realistic than ours. But of course, don't expect to ever generate any meaningful text, other than by random chance: all we are doing is \n",
    "sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there is \n",
    "a distinction between what communications are about, and the statistical structure of the messages in which communications are encoded. To \n",
    "evidence this distinction, here is a thought experiment: what if human language did a better job at compressing communications, much like \n",
    "our computers do with most of our digital communications? Then language would be no less meaningful, yet it would lack any intrinsic \n",
    "statistical structure, thus making it impossible to learn a language model like we just did.\n",
    "\n",
    "\n",
    "## Take aways\n",
    "\n",
    "* We can generate discrete sequence data by training a model to predict the next tokens(s) given previous tokens.\n",
    "* In the case of text, such a model is called a \"language model\" and could be based on either words or characters.\n",
    "* Sampling the next token requires balance between adhering to what the model judges likely, and introducing randomness.\n",
    "* One way to handle this is the notion of _softmax temperature_. Always experiment with different temperatures to find the \"right\" one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

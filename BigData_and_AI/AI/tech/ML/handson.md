<p align="right">Refer to Hands-on Machine Learning with Scikit-Learn & TensorFlow Aurelien Geron</p>

## 机器学习
- 机器学习: 计算机程序利用经验E学习任务T，性能是P，针对任务T的性能P随着经验E不断增长(让计算机具有学习的能力) ![AI](../../PNG/AI.PNG)
    - 擅长
        - 需要进行大量手工调整或需要拥有长串规则才能解决的问题
        - 问题复杂，传统方法难以解决
        - 环境有波动
        - 洞察复杂问题和大量数据
    - 挑战
        - 训练数据量不足
        - 没有代表性的训练数据：样本偏差
        - 低质量数据：数据清洗
        - 不相关特征：特征工程
        - 过拟合训练数据
            - 简化模型
                - 正则化：超参数
            - 收集更多训练数据
            - 减少训练数据的噪声
        - 欠拟合训练数据
            - 更强大的模型
            - 更好的特征
            - 减少限制

- 机器学习分类
    - 是否在人类监督下进行训练
        - 监督
            - 分类
                - 多分类
                - 多标签
                - 多输出
            - 回归
        - 非监督
            - 聚类
            - 可视化和降维
            - 异常检测
            - 关联性规则学习
        - 半监督 
            - 深度信念网络(DBN)：受限玻尔兹曼机
        - 强化学习：`Get reward or penalty`
            - 模拟环境
            - 马尔可夫决策过程
            - Q学习
    - 是否可以动态学习
        - 批量学习：使用所有可能数据进行训练
        - 在线学习
            - 学习速率
            - 性能监测
    - 简单比较数据或在训练数据中建立预测模型
        - 基于实例学习：记忆+相似度
        - 基于模型学习

## 一个完整的机器学习项目
- 明确问题、心怀愿景
    - 定义(商业)目标
    - 你的结果将会被怎么使用？
    - 当前的解决方法是什么？
    - 怎样划定这个问题(监督/非监督，在线/离线)？
    - 性能指标是什么？
    - 性能指标和(商业)目标一致吗？
    - 达到(商业)目标需要的最低性能指标是多少？
    - 有相关的问题吗？有可用的经验或工具吗？
    - 列出你和其他人所做的假设
    - 尽量核实假设
- 获取数据：尽量自动化，更好地获得新鲜数据
    - 列出你所需的数据以及数据量
    - 找到可以得到数据的地方
    - 预测数据占用的空间大小
    - 注意法律风险，如果需要，获得授权。
    - 接入API
    - 创建足够大的工作空间
    - 在不改变数据的情况下，转换数据为容易操作的类型
    - 确保敏感数据被删除或被保护(匿名)
    - 检查数据的大小和类型(时间序列，抽样数据，地理数据)
    - 取样出测试集，放在一边。
- 探究数据：咨询领域专家获得洞察力
    - 创建数据副本(若有需要，可以取样出合适的大小)
    - 用Jupyter notebook记录数据探究的过程
    - 研究每个特征及其特点
        - 名字
        - 类型(类别，数值，文本，结构化)
        - 缺失数据的百分比
        - 噪声及其类型(随机，异常值，舍入误差)
        - 可能对任务有用的特征？
        - 分布的种类(高斯，均匀，对数)
    - 对监督学习，指出目标特征
    - 可视化数据
    - 研究属性之间的关联
    - 想一想你会怎样手动解决这个问题
    - 找出你想要采用的有希望的数据变换
    - 是否需要额外数据(转到获取数据步骤)
    - 记录你所学到的知识，整理成文档
- 准备数据：在副本上进行，对数据变换编写函数
    - 数据清洗
        - 修正或移除异常值(可选)
        - 填充缺失值(0,均值，中位数)或删除所在列(行)
    - 特征选择(可选)
        - 删除对任务没有用的特征
    - 在合适的时机进行特征工程
        - 离散连续特征
        - 分解特征(类别，日期/时间)
        - 添加有希望的新特征(x^2,log(x))
        - 组合特征为新特征
    - 特征缩放：归一化或标准化
- 列出有希望的模型：取样数据使有合理的训练时间(牺牲复杂的模型)，尽量自动化
    - 快速使用默认参数训练多个粗糙的模型(线性，朴素贝叶斯，神经网络)
        - 测量和比较它们的性能
            - 采用交叉验证查看均值和方差
        - 对每个算法分析最重要的参数
        - 分析模型所犯的错误类型
            - 人类会用什么数据避免这些错误
        - 快速的特征选择和特征工程
        - 对前几步进行几次迭代
        - 列出最有希望的三到五个模型，最好错误类型不相同
    - 微调系统：尽量使用最多的数据，尽量自动化
        - 用网格搜索微调超参数
            - 将数据变换看成超参数(当你不确定时)
            - 采用随机网格搜索(除非超参数很少)
        - 尝试集成方法
        - 当你对最终模型有信心时，在测试集上估计泛化误差
    - 展示你的方法
        - 整理成文档
        - 准备良好的展示
            - 突出你的愿景
        - 解释你的方法为什么完成了(商业)目标
        - 展示在解决问题过程中有趣的点
            - 描述起作用和没起作用的方法
            - 列出你的假设和系统限制
        - 漂亮的可视化或简洁的陈述展示关键点
    - 部署
        - 为生产环境准备模型(接入生产数据，编写单元测试)
        - 编写监控代码评估你的系统的实时表现，在性能下降时预警
            - 当数据进化时，模型会逐渐腐烂
            - 可能需要人类流水线(通过外包服务)
            - 同时监控系统的输入质量
        - 在合理的新鲜数据上重新训练(尽量自动化)

## 常用算法
- 性能指标
    - 回归
        - 线性回归损失函数:范数指数越高,越关注大的值而忽略小的值
            - 均方根误差(RMSE)
            - 平均绝对误差(MAE)
        - 逻辑回归损失函数
        - 学习曲线:欠拟合还是过拟合
    - 分类
        - 精度(accuracy):对有偏差的数据集而言不是好的性能度量指标
        - 混淆矩阵(Confusion Matrix)
        - 准确率(precision) vs 召回率(recall): RP曲线 适合正例很少或关注假正例多于假反例
            - F1值
        - ROC曲线:面积(AUC)
        - 误差分析:找到一个不错的模型后,试图改善它,分析模型产生误差的类型
- 训练模型
    - 线性回归
        - 正规方程(Nomal Equation):$O(n^{2.4~3})$或$O(m)$
        - 梯度下降:特征缩放
            - 批量梯度下降
            - 随机梯度下降:`learning schedule`
            - 小批量梯度下降:GPU计算
        - 多项式回归
        - 正则化:训练过程
            - Ridge回归
            - Lasso回归:倾向于完全消除最不重要的特征的权重
            - Elastic Net
            - 早期停止法:`beautiful free lunch` 只有在验证误差高于最小值一段时间后
    - Logistic回归
        - 损失函数
        - 决策边界
    - Softmax回归:多类别Logistic回归
        - 损失函数:交叉熵
    - 支持向量机(SVM):特征缩放
        - 决策函数与预测:线性约束的凸二次规划问题
            - 对偶问题:样本数量比特征数量少
        - 分类:限制间隔违规的情况下,试图在两个类别之间找到尽可能大的街道
            - 软间隔: `C`越小,街道越宽,更多的间隔违规
            - 添加预测概率:使用交叉预测类别概率
            - 非线性分类
                - 添加多项式特征:产生大量的特征
                    - 核技巧: `kernel="poly"`
                - 增加相似特征:如何选择地标?
                    - 核技巧:`kernel="rbf", \gamma=0.1` 
            - 计算复杂度 ![SVM](../../PNG/svm.png)
        - 回归:限制间隔违规情况下,尽量放置更多的样本在街道上
        - 异常值检测
        - 在线支持向量机
    - 决策树 ![DT](../../PNG/iris_tree.png)
        - 分类:可以估计概率
            - 损失函数:找到单个特征和阈值最小化不纯度
                - Gini指数:测量属性的纯度
                - 熵(Entropy):信息增益
            - 算法:找到最优树是一个NP完全问题
                - CART:仅产生二叉树
                    - 训练:$O(nm\log(m))$
                    - 预测:$O(\log(m))$
                - ID3
            - 正则化:非参数模型,根据数据的特性参数自由生长
                - 参数限制模型
                    - 叶节点必须具有的最小样本
                    - 叶节点最大数量
                - 剪枝:当节点的全部子节点是,如果对纯度的提升不具有统计意义,此节点就是不重要的
                    - 假设检验:$\kappa^2$检测
            - 优缺点
                - 优点:不需要太多的数据预处理,尤其是不需要进行特征的缩放
                - 缺点:喜欢正交的决策边界,对训练数据的微小变化非常敏感
        - 回归:划分区域中实例的平均目标值
            - 最小化MSE
    - 集成学习和随机森林
        - 不同的训练算法得到一些不同的分类器
        - 使用相同的训练算法,但是在不同的训练集上去训练:可扩展性高,并行化(GPU)训练
            - Bagging:有放回采样
                - Out-of-Bag评价:平均下来,只有63%的训练实例被每个分类器取样
                - 高偏差换低方差
                    - 随机贴片(Patches):对训练实例和特征的采样
                    - 随机子空间:保留了所有的训练实例,但对特征采样
            - Pasting:无放回采样
        - 随机森林:决策树的集成
            - 分裂特征在一个随机的特征集中找最好的特征,高偏差换低方差
                - 极端随机树:对特征使用随机阈值,高偏差换低方差
            - 特征重要性:特征在森林的全部树中出现的平均深度
        - 提升(Boosting):顺序训练分类器,尝试修正前面的分类,不能并行化
            - Adaboost:对之前分类结果不对的训练实例多加关注
                - 实例权重更新
                - 预测:权重投票 
            - Gradient Boosting:新的分类器去拟合前面分类器预测的残差
                - GBDT
                - 正则化:shrinkage 每个树的贡献
                - 早期停止
            - XGBoost:考虑树的复杂度以及损失函数的二阶导师
            - Stacking:训练模型(blender)执行聚合
    - 朴素贝叶斯: 贝叶斯公式
- 无监督学习
    - 降维
        - 维数灾难
            - 高维超正方体中,大多数点都分布在边界处
            - 随机两点平均距离很远,造成过拟合
            - 达到给定密度所需训练实例城指数增长
        - 具体方法
            - 投影:找到接近数据集分布的超平面,将所有的数据都投影到这个超平面上
                - PCA
                    - 选择正确的超平面:保留最大方差
                    - 主成分:超平面的轴(SVD)
                    - 投影
                    - 方差解释率
                - 增量(Incremental)PCA:在线应用PCA
                - 随机PCA
                - 核PCA:高维特征空间中的线性决策边界对应于原始空间中的复杂非线性决策边界
                    - 选择核并调整超参数
                        - 视作监督学习的准备步骤,网格搜索
                        - 选择产生最低重建误差的核和超参数
            - 流形学习
                - 局部线性嵌入(LLE):测量每个训练实例与其最近邻之间的线性关系,然后寻找能最好地保留这些局部关系的训练集的低维表示
            - 其他算法
                - 多维缩放(MDS):尝试保持实例之间距离的同时降低了维度
                - Isomap:每个实例连接到最近的邻居来创建图形,然后在尝试保持实例之间的测地距离时降低维度
                - t-SNE:用于降低维​​度,同时试图保持相似的实例临近并将不相似的实例分开(主要用于可视化)
                - 线性判别分析(LDA):在使用其他分类算法前降维技术
    - 聚类
        - K-Means:随机选择K个随机的点,按照训练集距离K个中心点的距离将训练集分为K类
            - 优化目标:最小化所有的数据点与其所关联的聚类中心点之间的距离之和
            - 初始化
                - 随机初始化
                - K-Means++
            - 选择聚类数K
                - 成本函数对K的图像(elbow)
                - silhouette得分
            - 选择最佳目标:`inertia metric`(训练集与最近的中心距离之和)
            - 加速K-Means:`elkan`(三角不等式)
        - DBSCAN:基于密度
        - 谱聚类:基于图论
        - Agglomerative聚类:自底而上的层次聚类方法
        - Gaussian Mixtures:概率密度函数
            - 异常检测:处于低密度区域
            - 选择模型:Bayesian Information Criterion(BIC) Akaike Information Criterion(AIC)
    - 关联性规则学习
        - Apriori
        - Eclat

## 深度学习
- 兴起原因
    - 大量数据
    - 计算能力
    - 改进算法
    - 接近全局最优
    - 良性循环
- 感知器
    - 神经元的逻辑运算
- 深度学习(DNN): 两个或多个隐含层的多层感知器
    - 基本训练方法
        - 反向传播
            - 预测(向前)，测量误差
            - 反向遍历每个连接的误差贡献
            - 微调连接器权值减少误差
        - 激活函数
            - logistic函数
            - 双曲正切函数
            - Relu函数
        - 隐藏层
            - 隐藏层数量：分层结构(重用神经网络)
            - 每层的神经元数量：`黑色艺术`
                - 早期停止
                - drop out
    - 进阶
        - 问题：梯度消失/爆炸
            - 初始化策略(不同的激活函数): logistic$\rightarrow$Xavier
            - 非饱和激活函数：ELU
            - 批量标准化(BN)
            - 梯度裁剪
        - 预训练
            - 无监督预训练(RBM)
            - 自编码器
            - 在辅助任务上预训练
        - 复用预训练层：`Models Zoos`
            - 冻结较低层
            - 缓存冻结层
            - 调整、删除和替换较高层
        - 更快的优化器
            - 动量优化
            - Nesterov加速梯度
            - AdaGrad
            - RMSProp
            - Adam优化
        - 学习率调整
        - 正则化避免过拟合
            - 早期停止
            - L1和L2正则化
            - Dropout
            - 最大范数正则化
            - 数据增强

## 常见神经网络
- 卷积神经网络(CNN)：图像
    - 基础
        - 卷积层：`convolution`
            - 卷积核/过滤器: `padding`
            - 叠加的多个特征映射
            - 内存需求
        - 池化层：`pooling`
            - Max pooling
    - CNN架构
        - LeNet-5
        - AlexNet
        - GoogleNet
        - ResNet
    - 应用：计算机视觉

- 循环神经网络(RNN)：时间序列
    - 基本概念
        - 循环神经元
        - 记忆单元
        - 输入和输出序列
    - 训练RNN：`BPTT`
        - 训练序列分类器
        - 截断时间反向传播
            - LSTM单元
        - 生成RNN
        - 深度RNN
    - 应用: 自然语言处理
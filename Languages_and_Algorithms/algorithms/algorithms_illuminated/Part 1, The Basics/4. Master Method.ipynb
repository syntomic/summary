{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master method\n",
    "\n",
    "In this notebook we will look at Master Method, a technique used to determine the running time of recursive algorithms. We plug in some characteristics of the recursive algorithms and we get an upper bound on the running time of the algorithm.\n",
    "\n",
    "We will revist Recursive Integer multiplication and Karatsuba multiplication introduced in first chapter. To recap, we split two number x and y of n digits into smaller numbers a, b, c and d each of length $\\frac{n}{2}$.\n",
    "\n",
    "$x\\:=\\: a\\cdot 10^{\\frac{n}{2}} + b\\:\\:$ and $\\:\\:y\\:=\\: c\\cdot 10^{\\frac{n}{2}} + d\\:\\:$\n",
    "Thus\n",
    "\n",
    "$x.y = (a\\cdot 10^{\\frac{n}{2}} + b) \\cdot (c\\cdot 10^{\\frac{n}{2}} + d) = 10^n\\cdot (a\\cdot c) + 10^{\\frac{n}{2}}\\cdot (a\\cdot d + b\\cdot c) + b\\cdot d$\n",
    "\n",
    "Expressing $T(n)$ in terms of recursive calls we get\n",
    "\n",
    "$T(n) \\leq 4\\cdot T(\\frac{n}{2}) + O(n)$\n",
    "\n",
    "Since the input is halved on each recursive call, we recursively operator on 4 smaller problems. After the 4 recursive calls we do linear amount of work outside the recursive calls.\n",
    "\n",
    "For the karatsuba multiplication, we apply gausses trick to reduce one recursive call by representing two multiplication as a linear combination of others. Thus expressing $T(n)$ as above we get\n",
    "\n",
    "$T(n) \\leq 3\\cdot T(\\frac{n}{2}) + O(n)$\n",
    "\n",
    "Thus intutively, karatsuba miltiplication seems better than the standard divide and conquer recursive technique though the linear work to be performed has slightly more constant multiple than the recursive multiplication.\n",
    "\n",
    "---\n",
    "\n",
    "For master method, we express the recursive problems problem as follows\n",
    "\n",
    "$T(n)\\: \\leq \\:  a\\cdot T(\\frac{n}{b}) + O(n^d)$\n",
    "\n",
    "After several recursive calls, the base case will return the answer in constant time. The constants a, b and d are defined as\n",
    "\n",
    "a = Number of recursive calls on smaller input\n",
    "b = The input shrinkage factor\n",
    "d = exponent of running time for the work done in the combine phase after the recursive calls.\n",
    "\n",
    "The master method assumes that the input shrinks at a fixed rate and the smaller subproblems operate on a similar sized problems. This assumption holds true for a large category of divide and conquer problems. For example, in case of merge sort the input is halved and we have two recursive calls on smaller problems. Outside these recursive calls, we perform linear work. Thus for merge sort **a, b ** are **2** and **d** is **1**\n",
    "\n",
    "For master method we have\n",
    "\n",
    "$$T(n) = \n",
    "\\begin{cases}\n",
    "    O(n^d log\\: n)  & \\text{if } a = b^d \\\\\n",
    "    O(n^d)          & \\text{if } a < b^d \\\\\n",
    "    O(n^{log_b\\: a})& \\text{if } a > b^d\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The value $a, b > 0$ and $d\\geq 0$. The value of d being 0 shows constant work done outside the recursive calls.\n",
    "\n",
    "\n",
    "***Also these constants are not dependent on the value n***.\n",
    "\n",
    "With these cases defined, let us apply this to some of the algorithms we know.\n",
    "\n",
    "- Merge Sort: We know the Merge Sort runs in $O(nlogn)$ time. The value of a b and d for merge sort are 2, 2 and 1 respectively. Since $a = b^d$, we are in case 1 of master method and thus we get $O(n) = nlogn$\n",
    "\n",
    "- Binary Search: Binary search works on half the input size on each recursive call and only involves one recursive call as we either operate on the left or right half of the numbers depending on the number to be searched. Also the work done outside recursive is done in constant time. Thus we get value of a, b and d as 1, 2 and 0 respectively. This again is case 1 and we get the running time of $logn$\n",
    "\n",
    "- Recursive Multiplication: As we saw earlier the value of a, b and d 4, 2 and 1 respectively. This is case 3 of master method and thus we get $O(n) = n ^{log_2 4}$. Since $log_2 4 = 2$ we have $O(n) = n^2$\n",
    "\n",
    "- Karatsuba multiplication: In this case we saw that the value of a, b and d are 3, 2, 1 respectively. This again is case 3 and thus $O(n) = n ^{log_2 3}$. The value of $log_2 3 \\approx 1.59$ and thus we have the upper bound as $O(n) = n^{1.59}$. This te karatsuba multiplication performs multiplication in sub quadratic time.\n",
    "\n",
    "\n",
    "- For Matrix multiplication, we saw that any $n \\times n$ matrix is broken into two $\\frac{n}{2}\\times \\frac{n}{2}$ matrix. The work done outside is quardartic. Also we had 8 matrix multiplications of smaller inputs each of size $\\frac{n}{2}$. Thus, we have a, b and c as 8, 2 and 2 respectively. This again is case 3 and we have $O(n) = n ^{log_2 8}$. Since $log_2 8 = 3$ we have $O(n) = n^3$\n",
    "\n",
    "- Strassen's Matrix multiplication: This approach similar to gausses trick in Karatsuba multiplication, manages to reduce the number of recursive calls from 8 to 7. Thus we have a, b and c as 7, 2 and 2 respectively. Thus we have\n",
    " $O(n) = n ^{log_2 7}$. Since $log_2 7 \\approx 2.81$ we have $O(n) = n^{2.81}$\n",
    " \n",
    "- Fictitious case: The case is similar to merge sort where the input is split in two halves and we have two recursive calls, except that the merge after the recursice is quadratic and has time complexity of $O(n^2)$. Thus a, b and d = 2. This is the case where $a < b^d$ and thus is case 2 in which case the upper bound for the running time is $O(n^d) = O(n^2)$. Thus the time complexity is dominated by the merge/combine routine after the recursive calls.\n",
    "\n",
    "---\n",
    "\n",
    "All this sounds right but we havent proven them. In the following section we will be proving the claims made by master method.\n",
    "\n",
    "Let us approach this problem similar to how we deal with Merge sort. Suppose we have at each level a recursive calls, each acting on an input shrinking at a rate of b and the work outside recursive calls has quadratic of order d. \n",
    "\n",
    "Suppose the input is of size n, then at a given level j we have $a^j$ subproblems and each subproblem of size $n/b^j$. The total work $T(n)$ at a level j is then \n",
    "\n",
    "$$T(n_j) \\leq a ^ j\\cdot c\\cdot (\\frac{n}{b^j})^d = c\\cdot n^d\\cdot (\\frac{a}{b^d})^j$$\n",
    "\n",
    "With an input of size n, and the work shrinking at a rate of b, we have a total of $j = log_b n$ levels. Thus the total work done is \n",
    "\n",
    "$$T(n) \\leq c.n^d\\cdot \\sum_{j = 0}^{log_b n}(\\frac{a}{b^d})^j$$\n",
    "\n",
    "With the above equation, we now have three cases for various values of $a$ and $b^d$. The value a is also called *Rate of Sub Problem Proliferation (RSP)* and $b^d$ is called *Rate of Work Shrinkage (RWS)* \n",
    "\n",
    "---\n",
    "\n",
    "The simplest case is, $a = b^d$. In which case $\\sum_{j = 0}^{log_b n}(\\frac{a}{b^d})^j = \\sum_{j = 0}^{log_b n}1 = 1 + log_b n$\n",
    "\n",
    "Thus \n",
    "\n",
    "$T(n) \\leq c.n^d\\cdot (1 + log_bn) = O(n^d\\cdot log_b n)$ as the constants get suppressed in Big-Oh notation.\n",
    "\n",
    "---\n",
    "\n",
    "The case where *RSP* < *RWS*, that is $a < b^d$. Since a, b and d are constants, let us call $r = a/b^d$ and since $a < b^d$, we have r < 1. For a geometric sequence where r < 1 $\\sum_{j = 0}^{j = k}r^j \\leq \\frac{r}{1 - r}$\n",
    "\n",
    "Thus $T(n) \\leq c.n^d\\cdot \\sum_{j = 0}^{log_b n}(\\frac{a}{b^d})^j\\leq c.n^d\\cdot \\frac{r}{i - r}$. Since r is constant and constants are suppressed in Big Oh notations, we get $T(n) \\leq O(n^d)$\n",
    "\n",
    "---\n",
    "\n",
    "The third case is the one where we have $a > b^d$. For a geometric sequence r > 1 $\\sum_{j = 0}^{j = k}r^j \\leq \\frac{r}{1 - r}\\cdot r^k$\n",
    "\n",
    "Thus  $T(n) \\leq c.n^d\\cdot \\sum_{j = 0}^{log_b n}(\\frac{a}{b^d})^j\\leq c.n^d\\cdot \\frac{r}{i - r}\\cdot r^{log_b n}$\n",
    "\n",
    "Let us reduce $r^{log_b n}$\n",
    "\n",
    "$r^{log_b n} = (\\frac{a}{b^d})^{log_b n} = \\frac{a^{log_b n}}{(b ^ d) ^ {log_b n}}$\n",
    "\n",
    "Since $x^{log_w y} = y^{log_w x}$\n",
    "\n",
    "$\\frac{a^{log_b n}}{(b ^ d) ^ {log_b n}} = \\frac{n^{log_b a}}{n^{log_b b^d}}  = \\frac{n^{log_b a}}{n^d}$\n",
    "\n",
    "\n",
    "Thus  $T(n) \\leq c.n^d\\cdot \\sum_{j = 0}^{log_b n}(\\frac{a}{b^d})^j\\leq c.n^d\\cdot \\frac{r}{i - r}\\cdot r^{log_b n} = c\\cdot n^d \\cdot \\frac{n^{log_b a}}{n^d} = c\\cdot n^{log_b a}$\n",
    "\n",
    "Thus when $a > b^d$ we have $T(n) \\leq O(n^{log_b a})$\n",
    "\n",
    "\n",
    "\n",
    "We now have proven the claim of Master method for getting an the upper bounds on the running time for recursive algorithms.\n",
    "\n",
    "With the derivations out of the way, let us now solve the problems given in the book for exercise.\n",
    "\n",
    "---\n",
    "\n",
    "#### Problem 4.1\n",
    "\n",
    "For the three parameters a, b and d, the best interpretation of $b^d$ would be the rate at with work per sub problem shrinks. the value b can called as the rate of sub problem shrinkage\n",
    "\n",
    "---\n",
    "\n",
    "#### Problem 4.2\n",
    "\n",
    "Given that $T(n)\\leq 7\\cdot T(\\frac{n}{3}) + O(n^2)$, we have a = 7, b = 3 and d = 2 thus we come in case 2 since $a < b^d$ and the correct minimum upper bound would be $O(n^d) = O(n^2)$\n",
    "\n",
    "---\n",
    "\n",
    "#### Problem 4.3\n",
    "\n",
    "Given that $T(n)\\leq 9\\cdot T(\\frac{n}{3}) + O(n^2)$, we have a = 3, b = 3 and d = 2 and we come in case 1 and the correct minimum upper bound is $O(n^dlog\\:n) = O(n^2log\\:n)$\n",
    "\n",
    "---\n",
    "\n",
    "#### Problem 4.4\n",
    "\n",
    "Given that $T(n)\\leq 5\\cdot T(\\frac{n}{3}) + O(n)$, we have a = 5, b = 3 and d = 1, we thus come in case 3 as $a > b^d$ and the minimum upper bound is $O(n^{log_b a}) = O(n^{log_3 5}) = O(n^{1.47})$\n",
    "\n",
    "---\n",
    "\n",
    "#### Problem 4.5\n",
    "\n",
    "Suppose we have $T(n) \\leq T(\\sqrt n) + 1$, what would be the smallest correct upper bound. The problem isn't very easy solution wasn't clear immediately and I had to read the forums of the course on Coursera to get some hint. This is the explanation.\n",
    "\n",
    "- Represent n as $n = 2^{log_2 n}$\n",
    "- Talking squareroot of a number is same as halfing the exponent, that is $sqrt(x) = x^{\\frac{1}{2}}$ and $sqrt(sqrt(x)) = (x ^{\\frac{1}{2}})^ \\frac{1}{2} = x^{\\frac{1}{4}}$, thus $sqrt(n) = 2^{\\frac{log_2 n}{2}}$\n",
    "- We need to half a number approximately  $log_2 n$ times for it to reach just under 1. This is something we have seen in the analysis of of divide and conquer algorithms.\n",
    "- Thus for the number $2^{\\frac{log_2 n}{2}}$ to become 1, the term $\\frac{log_2 n}{2}$ needs to reduce to under 1 (the floor function ensures the value becomes 0 as soon as it drops under 1) by continously halfing it. We have seen in the previous point that a number needs to be halfed approximately $log_2 n$ times to go under 1, this $log_2 n$ needs to be halfed $log_2(log_2 n)$ times to become less than 1 and thus a reasonable upper bound of the problem is $O(log\\:log n)$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
